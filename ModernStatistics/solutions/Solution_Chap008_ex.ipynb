{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d3ac80",
   "metadata": {},
   "source": [
    "# Exercise Chapter 8 \n",
    "Modern Statistics: A Computer Based Approach with Python<br>\n",
    "by Ron Kenett, Shelemyahu Zacks, Peter Gedeck\n",
    "\n",
    "Publisher: Springer International Publishing; 1st edition (September 15, 2022) <br>\n",
    "ISBN-13: 978-3031075650\n",
    "\n",
    "(c) 2022 Ron Kenett, Shelemyahu Zacks, Peter Gedeck\n",
    "\n",
    "The code needs to be executed in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ['OUTDATED_IGNORE'] = '1'\n",
    "from outdated import OutdatedPackageWarning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=OutdatedPackageWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistat\n",
    "import networkx as nx\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc897e",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bad63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda import FDataGrid\n",
    "from skfda.representation.interpolation import SplineInterpolation\n",
    "\n",
    "dissolution = mistat.load_data('DISSOLUTION.csv')\n",
    "\n",
    "# convert the data to FDataGrid\n",
    "data = []\n",
    "labels = []\n",
    "names = []\n",
    "for label, group in dissolution.groupby('Label'):\n",
    "  data.append(group['Data'].values)\n",
    "  labels.append('Reference' if label.endswith('R') else 'Test')\n",
    "  names.append(label)\n",
    "labels = np.array(labels)\n",
    "grid_points = np.array(sorted(dissolution['Time'].unique()))\n",
    "fd = FDataGrid(np.array(data), grid_points,\n",
    "       dataset_name='Dissolution',\n",
    "       argument_names=['Time'],\n",
    "       coordinate_names=['Dissolution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.preprocessing.registration import ShiftRegistration\n",
    "shift_registration = ShiftRegistration()\n",
    "\n",
    "fd_registered = {}\n",
    "for order in (1, 2, 3):\n",
    "    fd.interpolation = SplineInterpolation(interpolation_order=order)\n",
    "    fd_registered[order] = shift_registration.fit_transform(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c771a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.exploratory import stats\n",
    "\n",
    "group_colors = {'Reference': 'grey', 'Test': 'black'}\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(8, 3))\n",
    "for ax, order in zip(axes, (1, 2, 3)):\n",
    "    mean_ref = stats.mean(fd_registered[order][labels=='Reference'])\n",
    "    mean_test = stats.mean(fd_registered[order][labels=='Test'])\n",
    "    means = mean_ref.concatenate(mean_test)\n",
    "    means.plot(axes=[ax], group=['Reference', 'Test'], group_colors=group_colors)\n",
    "    ax.set_title(f'Order {order}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4a7f0",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda import FDataGrid\n",
    "\n",
    "pinchraw = skfda.datasets.fetch_cran('pinchraw', 'fda')['pinchraw']\n",
    "pinchtime = skfda.datasets.fetch_cran('pinch', 'fda')['pinchtime']\n",
    "\n",
    "fd = FDataGrid(pinchraw.transpose(), pinchtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fd.plot()\n",
    "ax = fig.axes[0]\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Pinch force')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from skfda.preprocessing.smoothing.kernel_smoothers import NadarayaWatsonSmoother\n",
    "\n",
    "def plotSmoothData(fd, smoothing_parameter, ax):\n",
    "    smoother = NadarayaWatsonSmoother(smoothing_parameter=smoothing_parameter)\n",
    "    fd_smooth = smoother.fit_transform(fd)\n",
    "    _ = fd_smooth.plot(axes=[ax])\n",
    "    ax.set_title(f'Smoothing parameter {smoothing_parameter}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Pinch force')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2)\n",
    "axes = list(itertools.chain(*axes))  # flatten list of lists\n",
    "for i, sp in enumerate([0.03, 0.01, 0.001, 0.0001]):\n",
    "    plotSmoothData(fd, sp, axes[i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother = NadarayaWatsonSmoother(smoothing_parameter=0.005)\n",
    "fd_smooth = smoother.fit_transform(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = fd_smooth.data_matrix.argmax(axis=1)\n",
    "landmarks = [pinchtime[idx] for idx in max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ab006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.preprocessing.registration import landmark_shift\n",
    "fd_landmark = landmark_shift(fd_smooth, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edbd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fd_landmark.plot()\n",
    "ax = fig.axes[0]\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Pinch force')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5473b2",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e656005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c570f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "\n",
    "moisturespectrum = skfda.datasets.fetch_cran('Moisturespectrum', 'fds')\n",
    "moisturevalues = skfda.datasets.fetch_cran('Moisturevalues', 'fds')\n",
    "\n",
    "frequencies = moisturespectrum['Moisturespectrum']['x']\n",
    "spectra = moisturespectrum['Moisturespectrum']['y']\n",
    "moisture = moisturevalues['Moisturevalues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac16f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.Series(moisture).hist(bins=20, color='grey', label='Moisture content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd48cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "moisture_class = ['high' if m > 14.5 else 'low' for m in moisture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e225937",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = spectra.transpose()\n",
    "fd = skfda.FDataGrid(intensities, frequencies)\n",
    "\n",
    "# divide each sample spectrum by it's mean intensities\n",
    "intensities_normalized = (intensities - intensities.mean(dim='dim_0')) / intensities.std(dim='dim_0')\n",
    "fd_normalized = skfda.FDataGrid(intensities_normalized, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "_ = fd.plot(axes=axes[0], alpha=0.5,\n",
    "            # color lines by moisture class\n",
    "            group=moisture_class, group_names={'high': 'high', 'low': 'low'})\n",
    "_ = fd_normalized.plot(axes=axes[1], alpha=0.5,\n",
    "            group=moisture_class, group_names={'high': 'high', 'low': 'low'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec37073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.ml.classification import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracies = []\n",
    "for rs in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fd,\n",
    "        moisture_class, random_state=rs, test_size=0.5)\n",
    "    knn_original = KNeighborsClassifier()\n",
    "    knn_original.fit(X_train, y_train)\n",
    "    acc_original = accuracy_score(y_test, knn_original.predict(X_test))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fd_normalized,\n",
    "        moisture_class, random_state=rs, test_size=0.5)\n",
    "    knn_normalized = KNeighborsClassifier()\n",
    "    knn_normalized.fit(X_train, y_train)\n",
    "    acc_normalized = accuracy_score(y_test, knn_normalized.predict(X_test))\n",
    "    accuracies.append({\n",
    "        'original': acc_original,\n",
    "        'normalized': acc_normalized,\n",
    "    })\n",
    "accuracies = pd.DataFrame(accuracies)\n",
    "ax  = accuracies.plot.scatter(x='original', y='normalized')\n",
    "_ = ax.plot([0.7, 0.9], [0.7, 0.9], color='black')\n",
    "ax.set_xlabel('Accuracy of models based on original spectra')\n",
    "ax.set_ylabel('Accuracy of models based on normalized spectra')\n",
    "plt.show()\n",
    "\n",
    "# mean of accuracies\n",
    "mean_accuracy = accuracies.mean()\n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3ec3c",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.ml.regression import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = []\n",
    "for rs in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fd,\n",
    "        moisture, random_state=rs, test_size=0.5)\n",
    "    knn_original = KNeighborsRegressor()\n",
    "    knn_original.fit(X_train, y_train)\n",
    "    mae_original = mean_absolute_error(y_test, knn_original.predict(X_test))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(fd_normalized,\n",
    "        moisture, random_state=rs, test_size=0.5)\n",
    "    knn_normalized = KNeighborsRegressor()\n",
    "    knn_normalized.fit(X_train, y_train)\n",
    "    mae_normalized = mean_absolute_error(y_test, knn_normalized.predict(X_test))\n",
    "    mae.append({\n",
    "        'original': mae_original,\n",
    "        'normalized': mae_normalized,\n",
    "    })\n",
    "mae = pd.DataFrame(mae)\n",
    "ax  = mae.plot.scatter(x='original', y='normalized')\n",
    "ax.plot([0.3, 1.0], [0.3, 1.0], color='black')\n",
    "ax.set_xlabel('MAE of models based on original spectra')\n",
    "ax.set_ylabel('MAE of models based on normalized spectra')\n",
    "plt.show()\n",
    "\n",
    "# mean of MAE\n",
    "mean_mae = mae.mean()\n",
    "mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_normalized.predict(X_test)\n",
    "predictions = pd.DataFrame({'actual': y_test, 'predicted': y_pred})\n",
    "minmax = [min(*y_test, *y_pred), max(*y_test, *y_pred)]\n",
    "\n",
    "ax = predictions.plot.scatter(x='actual', y='predicted')\n",
    "ax.set_xlabel('Moisture content')\n",
    "ax.set_ylabel('Predicted moisture content')\n",
    "ax.plot(minmax, minmax, color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e05415",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b654b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.preprocessing.dim_reduction.projection import FPCA\n",
    "\n",
    "fpca_original = FPCA(n_components=2)\n",
    "_ = fpca_original.fit(fd)\n",
    "\n",
    "fpca_normalized = FPCA(n_components=2)\n",
    "_ = fpca_normalized.fit(fd_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFPCA(fpca, fd, ax):\n",
    "  fpca_df = pd.DataFrame(fpca.transform(fd))\n",
    "  fpca_df.plot.scatter(x=0, y=1,\n",
    "      c=['C1' if mc == 'high' else 'C2' for mc in moisture_class], ax=ax)\n",
    "  ax.set_xlabel('Component 1')\n",
    "  ax.set_xlabel('Component 2')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=[6, 3])\n",
    "plotFPCA(fpca_original, fd, axes[0])\n",
    "plotFPCA(fpca_normalized, fd_normalized, axes[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462600e",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistat.nlp import globalWarmingBlogs\n",
    "blogs = globalWarmingBlogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c20deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "labels = []\n",
    "for blog, text in blogs.items():\n",
    "  for paragraph in text.split('\\n'):\n",
    "    paragraph = paragraph.strip()\n",
    "    if not paragraph: # ignore empty paragraphs\n",
    "      continue\n",
    "    paragraphs.append(paragraph)\n",
    "    labels.append(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28aeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d[\\d,]*', '', text)\n",
    "    text = '\\n'.join(line for line in text.split('\\n')\n",
    "                     if not line.startswith('ntsb'))\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor,\n",
    "                             stop_words='english')\n",
    "counts = vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "print('shape of DTM', counts.shape)\n",
    "print('total number of terms', np.sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "termCounts = np.array(counts.sum(axis=0)).flatten()\n",
    "topCounts = termCounts.argsort()\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for n in reversed(topCounts[-10:]):\n",
    "  print(f'{terms[n]} & {termCounts[n]} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee60d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "svd = TruncatedSVD(5)\n",
    "norm_tfidf = Normalizer().fit_transform(tfidf)\n",
    "lsa_tfidf = svd.fit_transform(norm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "data = {}\n",
    "for i, component in enumerate(svd.components_, 1):\n",
    "  compSort = component.argsort()\n",
    "  idx = list(reversed(compSort[-10:]))\n",
    "  data[f'Topic {i}'] = [terms[n] for n in idx]\n",
    "  data[f'Loading {i}'] = [component[n] for n in idx]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{\\\\tiny\")\n",
    "\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(10)\n",
    "norm_tfidf = Normalizer().fit_transform(tfidf)\n",
    "lsa_tfidf = svd.fit_transform(norm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5aefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "data = {}\n",
    "for i, component in enumerate(svd.components_, 1):\n",
    "  compSort = component.argsort()\n",
    "  idx = list(reversed(compSort[-10:]))\n",
    "  data[f'Topic {i}'] = [terms[n] for n in idx]\n",
    "  data[f'Loading {i}'] = [component[n] for n in idx]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{\\\\tiny\")\n",
    "print(df.iloc[:, :10].style.format(precision=2).hide(axis='index').to_latex(hrules=True))\n",
    "\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761dbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "blog1 = [label == 'blog-1' for label in labels]\n",
    "blog2 = [label == 'blog-2' for label in labels]\n",
    "ax.plot(lsa_tfidf[blog1, 0], lsa_tfidf[blog1, 1], 'ro')\n",
    "ax.plot(lsa_tfidf[blog2, 0], lsa_tfidf[blog2, 1], 'go')\n",
    "ax.set_xlabel('First component')\n",
    "ax.set_xlabel('Second component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f1433",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4db624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistat.nlp import covid19Blogs\n",
    "blogs = covid19Blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "labels = []\n",
    "for blog, text in blogs.items():\n",
    "  for paragraph in text.split('\\n'):\n",
    "    paragraph = paragraph.strip()\n",
    "    if not paragraph:\n",
    "      continue\n",
    "    paragraphs.append(paragraph)\n",
    "    labels.append(blog)\n",
    "\n",
    "def preprocessor(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'\\d[\\d,]*', '', text)\n",
    "  text = '\\n'.join(line for line in text.split('\\n')\n",
    "                   if not line.startswith('ntsb'))\n",
    "  return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor, stop_words='english')\n",
    "counts = vectorizer.fit_transform(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae760ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(10)\n",
    "tfidf = Normalizer().fit_transform(tfidf)\n",
    "lsa_tfidf = svd.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "data = {}\n",
    "for i, component in enumerate(svd.components_, 1):\n",
    "  compSort = component.argsort()\n",
    "  idx = list(reversed(compSort[-10:]))\n",
    "  data[f'Topic {i}'] = [terms[n] for n in idx]\n",
    "  data[f'Loading {i}'] = [component[n] for n in idx]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34031c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{\\\\tiny\")\n",
    "print(df.iloc[:, :10].style.format(precision=2).hide(axis='index').to_latex(hrules=True))\n",
    "\"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for blog in blogs:\n",
    "    match = [label == blog for label in labels]\n",
    "    ax.plot(lsa_tfidf[match, 0], lsa_tfidf[match, 1], 'o', label=blog)\n",
    "ax.legend()\n",
    "ax.set_xlabel('First component')\n",
    "ax.set_xlabel('Second component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02e07d",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mistat.load_data('LAPTOP_REVIEWS')\n",
    "data['Review'] = data['Review title'] + ' ' + data['Review content']\n",
    "reviews = data.dropna(subset=['User rating', 'Review title', 'Review content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51138498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d[\\d,]*', '', text)\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor,\n",
    "                             stop_words='english')\n",
    "counts = vectorizer.fit_transform(reviews['Review'])\n",
    "print('shape of DTM', counts.shape)\n",
    "print('total number of terms', np.sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "svd = TruncatedSVD(20)\n",
    "tfidf = Normalizer().fit_transform(tfidf)\n",
    "lsa_tfidf = svd.fit_transform(tfidf)\n",
    "print(lsa_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "outcome = ['positive' if rating == 5 else 'negative'\n",
    "           for rating in reviews['User rating']]\n",
    "\n",
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, outcome,\n",
    "                                                test_size=0.4, random_state=1)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "accuracy = accuracy_score(ytest, logit_reg.predict(Xtest))\n",
    "print(accuracy)\n",
    "confusion_matrix(ytest, logit_reg.predict(Xtest))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
